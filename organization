package Organization;
import org.slf4j.Logger
import org.slf4j.LoggerFactory
import crowdsourcing.AdjCosineSimilarity;
import util.ExperimentConfigGenerator;
import util.WeightLearner;
import com.google.common.collect.Iterables
import edu.umd.cs.psl.application.inference.MPEInference
import edu.umd.cs.psl.application.learning.weight.maxmargin.MaxMargin.LossBalancingType
import edu.umd.cs.psl.application.learning.weight.maxmargin.MaxMargin.NormScalingType
import edu.umd.cs.psl.config.*
import edu.umd.cs.psl.database.DataStore
import edu.umd.cs.psl.database.Database
import edu.umd.cs.psl.database.DatabasePopulator
import edu.umd.cs.psl.database.Partition
import edu.umd.cs.psl.database.ResultList
import edu.umd.cs.psl.database.rdbms.RDBMSDataStore
import edu.umd.cs.psl.database.rdbms.driver.H2DatabaseDriver
import edu.umd.cs.psl.database.rdbms.driver.H2DatabaseDriver.Type
import edu.umd.cs.psl.evaluation.result.FullInferenceResult
import edu.umd.cs.psl.evaluation.statistics.ContinuousPredictionComparator
import edu.umd.cs.psl.groovy.*
import edu.umd.cs.psl.model.argument.ArgumentType
import edu.umd.cs.psl.model.argument.GroundTerm
import edu.umd.cs.psl.model.argument.UniqueID
import edu.umd.cs.psl.model.argument.Variable
import edu.umd.cs.psl.model.atom.GroundAtom
import edu.umd.cs.psl.model.atom.QueryAtom
import edu.umd.cs.psl.model.atom.RandomVariableAtom
import edu.umd.cs.psl.model.kernel.CompatibilityKernel
import edu.umd.cs.psl.model.parameters.Weight
import edu.umd.cs.psl.ui.loading.*
import edu.umd.cs.psl.util.database.Queries

def dataPath = "./data/"
Logger log = LoggerFactory.getLogger(this.class)
ConfigManager cm = ConfigManager.getManager();
ConfigBundle cb = cm.getBundle("crowdsourcing");
def defPath = System.getProperty("java.io.tmpdir") + "/psl-yelp"
def dbpath = cb.getString("dbpath", defPath)
DataStore data = new RDBMSDataStore(new H2DatabaseDriver(Type.Disk, dbpath, true), cb)
ExperimentConfigGenerator configGenerator = new ExperimentConfigGenerator("crowdsourcing");
methods = ["MLE"];
configGenerator.setLearningMethods(methods);
configGenerator.setVotedPerceptronStepCounts([100]);
configGenerator.setVotedPerceptronStepSizes([(double) 1.0]);


/*** MODEL DEFINITION ***/
log.info("Initializing model ...");
PSLModel m = new PSLModel(this, data);


m.add predicate: "org" , types: [ArgumentType.UniqueID]\\twitter user id(org)
m.add predicate: "orgFriends" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID]\\user id (org), user id (org) : to be inferred
m.add predicate: "OrgCompetitors" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID]\\user id (org), user id (org) : to be inferred
m.add predicate: "OrgNeutral" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID]\\user id (org), user id (org) : to be inferred

m.add predicate: "twitterUser" , types: [ArgumentType.UniqueID] \\twitter user id : both org and followers
m.add predicate: "tweet" , types: [ArgumentType.UniqueID] \\ tweet id
\\m.add predicate: "followerTweet" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\Argument : tweet id,userid 
m.add predicate: "OrgTweet" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\Argument : tweet id,userid (org)
m.add predicate: "containsOrg" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\tweet id, userid (org)
m.add predicate: "posTweet" , types: [ArgumentType.UniqueID] \\ Argument :tweet id
m.add predicate: "negTweet" , types: [ArgumentType.UniqueID] \\ Argument :tweet id
m.add predicate: "neutralTweet" , types: [ArgumentType.UniqueID] \\ Argument :tweet id
\\m.add predicate: "orgFollower" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID]\\user id, user id (org): may be inferred 
m.add predicate: "group", types: [ArgumentType.UniqueID]
m.add predicate: "ally",types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\user id, user id
m.add predicate: "anti",types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\user id, user id
m.add predicate: "hasGroup",types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\user id, user id
m.add predicate: "sameGroup",types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\user id, user id
\\m.add predicate: "friends" , types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\user id, user id : not requied
\\m.add predicate: "similarFollower", types: [ArgumentType.UniqueID, ArgumentType.UniqueID] \\ user id, user id : not required
\\but will be useful to explore a followers' profile,recommend which org to follow, or in case the follower base is too small

\\baseline
m.add rule: ( org(O1) & org(O2) & tweet(T) & orgTweet(T,U) & containsOrg(T,O2) & posTweet(T) ) >> orgFriends(O1,O2), weight: 5.0;
m.add rule: ( org(O1) & org(O2) & tweet(T) & orgTweet(T,U) & containsOrg(T,O2) & negTweet(T) ) >> orgCompetitors(O1,O2), weight: 5.0;
\\Group based
m.add rule: ( org(O1) & org(O2) & group(G1) & group(G2) & hasGroup(O1,G1) & hasGroup(O2,G1))>> orgFriends(O1,O2)
m.add rule: ( org(O1) & org(O2) & group(G1) & group(G2) & hasGroup(O1,G1) & hasGroup(O2,G2) & ally(G1,G2))>>orgFriends(O1,O2)
m.add rule: ( org(O1) & org(O2) & group(G1) & group(G2) & hasGroup(O1,G1) & hasGroup(O2,G2) & anti(G1,G2))>>orgCompetitors(O1,O2)
\\Latent group identification

\\m.add rule: ( org(O1) & org(O2) & orgFollower(U,O1) & tweet(T) & followerTweet(T,U) & tweetContainsOrgname(T,O2) & neutralTweet(T) ) >> orgNeutral(O1,O2), weight: 5.0;
\\m.add rule: (orgFollower(U1) & twitterUser(U2) & friends(U1,U2)) >> orgFollower(U2), weight: 5.0; \\ may need this rule if we don't have enough followers
\\m.add rule: (orgFollower(U1) & twitterUser(U2) & similarFollower(U1,U2)) >> orgFollower(U2), weight: 5.0; \\ may need this rule if we don't have enough followers



log.info("Model: {}", m)
/* get all default weights */
Map<CompatibilityKernel,Weight> initWeights = new HashMap<CompatibilityKernel, Weight>()
for (CompatibilityKernel k : Iterables.filter(m.getKernels(), CompatibilityKernel.class))
initWeights.put(k, k.getWeight());
methods = ["MLE"];
configGenerator.setLearningMethods(methods);
/* MLE/MPLE options */
configGenerator.setVotedPerceptronStepCounts([100]);
configGenerator.setVotedPerceptronStepSizes([(double) 1.0]);
folds = 1
simThresh= 0.5
// assigning configbundle to the experiment, creating maps to keep results
List<ConfigBundle> configs = configGenerator.getConfigs();
Map<ConfigBundle,ArrayList<Double>> expResults = new HashMap<String,ArrayList<Double>>();
for (ConfigBundle config : configs) {
expResults.put(config, new ArrayList<Double>(folds));
}
// insert data in training and test partitions, each of which has read, write and labels(i.e. given values to compare with inferred values)
for (int fold = 0; fold < folds; fold++) {
Partition read_tr = new Partition(0 + fold * folds);
Partition write_tr = new Partition(1 + fold * folds);
Partition read_te = new Partition(2 + fold * folds);
Partition write_te = new Partition(3 + fold * folds);
Partition labels_tr = new Partition(4 + fold * folds);
Partition labels_te = new Partition(5 + fold * folds);


def inserter;
// users
inserter = data.getInserter(user, read_tr);
InserterUtils.loadDelimitedData(inserter, dataPath + "/users-tr.txt");// need to change
inserter = data.getInserter(user, read_te);
InserterUtils.loadDelimitedData(inserter, dataPath + "/users-te.txt");// need to change

inserter = data.getInserter(rating, read_te);
InserterUtils.loadDelimitedDataTruth(inserter, dataPath + "/ratings/yelp-1-te-obs-" + fold + ".txt");

inserter = data.getInserter(rating, labels_tr);
InserterUtils.loadDelimitedDataTruth(inserter, dataPath + "/ratings/yelp-1-tr-uno-" + fold + ".txt");
log.info("{} \t", fold);
inserter = data.getInserter(rating, labels_te);
InserterUtils.loadDelimitedDataTruth(inserter, dataPath + "/ratings/yelp-1-te-uno-" + fold + ".txt");



/** POPULATE DB ***/
/* We want to populate the database with all groundings 'rating'
* To do so, we will query for all users and bussiness in train/test, then use the
* database populator to compute the cross-product.
**********check **************************************
*/
DatabasePopulator dbPop;
Variable User = new Variable("User");
Variable Business = new Variable("Business");
Set<GroundTerm> users = new HashSet<GroundTerm>();
Set<GroundTerm> businesses = new HashSet<GroundTerm>();
Map<Variable, Set<GroundTerm>> subs = new HashMap<Variable, Set<GroundTerm>>();
subs.put(User, users);
subs.put(Business, businesses);
def toClose;
AdjCosineSimilarity userCosSim = new AdjCosineSimilarity(rating, 1, ratingPrior, simThresh);
AdjCosineSimilarity businessCosSim = new AdjCosineSimilarity(rating, 0, businessAvgRating, simThresh);
Database trainDB = data.getDatabase(read_tr);
ResultList userGroundings = trainDB.executeQuery(Queries.getQueryForAllAtoms(user));
for (int i = 0; i < userGroundings.size(); i++) {
GroundTerm u = userGroundings.get(i)[0];
users.add(u);// adding u to the Map of users



log.info("Populating training database ...");
toClose = [user,business,rating,ratingPrior,businessAvgRating,friends,simObsRatingB,simObsRatingU] as Set// check
trainDB = data.getDatabase(write_tr, toClose, read_tr);
dbPop = new DatabasePopulator(trainDB);
dbPop.populate(new QueryAtom(rating, User, Business), subs);
Database labelsDB = data.getDatabase(labels_tr, [rating] as Set)


Database testDB = data.getDatabase(read_te);
userGroundings = testDB.executeQuery(Queries.getQueryForAllAtoms(user));
for (int i = 0; i < userGroundings.size(); i++) {
GroundTerm u = userGroundings.get(i)[0];
users.add(u);// a


/* Populate testing database. */
log.info("Populating testing database ...");
toClose = [user,business,rating,ratingPrior,businessAvgRating,friends,simObsRatingB,simObsRatingU] as Set;// check with sachi
testDB = data.getDatabase(write_te, toClose, read_te);
dbPop = new DatabasePopulator(testDB);
dbPop.populate(new QueryAtom(rating, User, Business), subs);
// no labels?
testDB.close();


*** EXPERIMENT ***////check
log.info("Starting experiment ...");
for (int configIndex = 0; configIndex < configs.size(); configIndex++) {
ConfigBundle config = configs.get(configIndex);
def configName = config.getString("name", "");
def method = config.getString("learningmethod", "");
/* Weight learning */
WeightLearner.learn(method, m, trainDB, labelsDB, initWeights, config, log)
log.info("Learned model {}: \n {}", configName, m.toString())
/* Inference on test set */
Database predDB = data.getDatabase(write_te, toClose, read_te);
Set<GroundAtom> allAtoms = Queries.getAllAtoms(predDB, rating)
for (RandomVariableAtom atom : Iterables.filter(allAtoms, RandomVariableAtom))
atom.setValue(0.0)
/* For discrete MRFs, "MPE" inference will actually perform marginal inference */
MPEInference mpe = new MPEInference(m, predDB, config)
FullInferenceResult result = mpe.mpeInference()
log.info("Objective: {}", result.getTotalWeightedIncompatibility())
predDB.close();

/* Evaluation *///check
predDB = data.getDatabase(write_te);
Database groundTruthDB = data.getDatabase(labels_te, [rating] as Set)
def comparator = new ContinuousPredictionComparator(predDB)
comparator.setBaseline(groundTruthDB)
def metrics = [ContinuousPredictionComparator.Metric.MSE, ContinuousPredictionComparator.Metric.MAE]
double [] score = new double[metrics.size()]
for (int i = 0; i < metrics.size(); i++) {
comparator.setMetric(metrics.get(i))
score[i] = comparator.compare(rating)
}
log.info("Fold {} : {} : MSE {} : MAE {}", fold, configName, score[0], score[1]);
expResults.get(config).add(fold, score);
predDB.close();
groundTruthDB.close()
}
trainDB.close()
}
log.info("\n\nRESULTS\n");
for (ConfigBundle config : configs) {
def configName = config.getString("name", "")
def scores = expResults.get(config);
for (int fold = 0; fold < folds; fold++) {
def score = scores.get(fold)
log.info("{} \t{}\t{}\t{}", configName, fold, score[0], score[1]);
log.Debug("{} \t{}\t{}\t{}", configName, fold, score[0], score[1]);// added
}
}
